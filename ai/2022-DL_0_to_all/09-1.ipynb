{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d51788-05a4-452e-adde-6a3f3a1113cb",
   "metadata": {},
   "source": [
    "# Lab-09-1 ReLU\n",
    "\n",
    "- Problem of Sigmoid\n",
    "- ReLU\n",
    "- Optimizer in PyTorch\n",
    "- Review: MNIST\n",
    "- Code: mnist_softmax\n",
    "- Code: mnist_nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943e8d6-b58e-4a08-b305-583c9608884d",
   "metadata": {},
   "source": [
    "## Problem of Sigmoid\n",
    "\n",
    "Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac94ce-8197-42c7-8a82-7ac5d66d0839",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "$f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bcbf5-0b01-4b6c-b7a8-ded4e2a87aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.sigmoid(x)\n",
    "x = torch.nn.relu(x)\n",
    "x = torch.nn.tanh(x)\n",
    "x = torch.nn.leaky_relu(x, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10685da-6526-4075-9ad7-a3c20873b938",
   "metadata": {},
   "source": [
    "## Optimizer in PyTorch\n",
    "\n",
    "- torch.optim.SGD\n",
    "- torch.optim.Adadelta\n",
    "- torch.optim.Adagrad\n",
    "- torch.optim.Adam\n",
    "- torch.optim.SparseAdam\n",
    "- torch.optim.Adamax\n",
    "- torch.optim.ASGD\n",
    "- torch.optim.LBFGS\n",
    "- torch.optim.RMSprop\n",
    "- torch.optim.Rprop\n",
    "\n",
    "https://www.slideshare.net/yongho/ss-79607172 : optimizer를 산 내려오는 작은 오솔길 잘 찾기에 비유해서 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec74de-6ecf-4524-a855-9cbd51cb8514",
   "metadata": {},
   "source": [
    "## Review: reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272054c-2239-401e-a874-a0e80e8749f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "...\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "data_loader = torch.utils.DataLoader(DataLoader=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "...\n",
    "for epoch in range(training_epochs):\n",
    "    ...\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d1e44-5d57-4be5-8b7c-0f3f1aeb613d",
   "metadata": {},
   "source": [
    "## Code: mnist_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1891b0b-1818-4fb8-9d22-c1c85c53214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "...\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)     # 10: 0-9\n",
    "\n",
    "# Initialization\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)   # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(linear.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54513d-c1b4-4e64-8c0e-d9c481d16281",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe88c3-50ec-4f45-90f4-5a0686ad0329",
   "metadata": {},
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost = \", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad8bef-a412-4183-b50b-8fbd0b80caeb",
   "metadata": {},
   "source": [
    "## Code: mnist_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895bd06-efdb-424a-b4a8-0fd112802f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "...\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True).to(device)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True).to(device)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True).to(device)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# Initialization\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)     # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118e9e6-a063-49ba-983b-bee658eaf8c2",
   "metadata": {},
   "source": [
    "학습은 위 코드와 동일\n",
    "\n",
    "1개 레이어보다 3개 레이어에 ReLU를 사용하는 것이 더 좋다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
