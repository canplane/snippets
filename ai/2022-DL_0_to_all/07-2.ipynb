{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681a6f16-a7d1-4fd9-be96-ed6b68109e2f",
   "metadata": {},
   "source": [
    "# Lab-07-2 MNIST Introduction\n",
    "\n",
    "* What is MNIST?\n",
    "* Code: MNIST Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5a181-c61a-4eab-b099-6970e036a5a5",
   "metadata": {},
   "source": [
    "## What is MNIST?\n",
    "MNIST: handwritten digits dataset\n",
    "\n",
    "training set images 60000장, test set images 10000장.\n",
    "\n",
    "src: http://yann.lecun.com/exdb/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c0361-0923-4435-8a5b-220452ac1307",
   "metadata": {},
   "source": [
    "## Example of MNIST\n",
    "\n",
    "- 28 x 28 image\n",
    "- 1 channel gray image\n",
    "- 0 ~ 9 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a4cd7-64b6-4cbd-8fd1-30ae8789839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in data_loader:\n",
    "    # reshape input image into [batch_size by 784]\n",
    "    # label is not one-hot encoded\n",
    "    X = X.view(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c835396-7fb5-4252-9e5c-64ceb5da22f1",
   "metadata": {},
   "source": [
    "## torchvision\n",
    "The <u>torchvision</u> package consists of <u>popular datasets</u>, <u>model architectures</u>, and <u>common image transformations</u> for computer vision.\n",
    "\n",
    "- torchvision.datasets\n",
    "    - MNIST, Fashion-MNIST, EMNIST, COCO, LSUN, ...\n",
    "- torchvision.models\n",
    "    - Alexnet, AGG, ResNet, SqueezeNet, DenseNet, Inception v3\n",
    "- torchvision.transforms\n",
    "    - Transforms on PIL Image, Transforms on torch.*Tensor, Conversion Transforms, ...\n",
    "- torchvision.utils\n",
    "\n",
    "*(reference: https://pytorch.org/docs/stable/torchvision/index.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3441b6-f9e1-47f1-9e5e-2d7fc16e2bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp39-cp39-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: numpy in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: torch==1.13.0 in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sanghoon/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566b8d4-5b38-407f-a002-98440d0bf691",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading data\n",
    "\n",
    "- torchvision.datasets.MNIST 인자 설명\n",
    "    - root: 어느 경로에 MNIST 데이터가 있느냐?\n",
    "    - train: True면 MNIST에 있는 train set, False로 하면 test set을 불러오기.\n",
    "    - transform: MNIST를 불러올 때 어떤 transform을 적용할 거냐?\n",
    "        - 일반적으로 PyTorch의 경우, 이미지는 0-1 사이의 값을 갖게 되고, 순서는 채널, 높이, 너비. ([C, H, W])\n",
    "        - 일반적인 이미지는, 0-255, H, W, C 순서로 되어 있음.\n",
    "    - download: MNIST 데이터가 없으면 다운을 받겠다.\n",
    "\n",
    "- torch.utils.data.Dataset 인자 설명\n",
    "    - DataLoader: 어떤 데이터를 로드할 거냐?\n",
    "    - batch_size: 이 mnist_train 이미지를 불러올 때, 몇 개씩 짤라서 불러올래? (여기선 100을 사용하겠다.)\n",
    "    - shuffle: 60000장에서 우리가 100장씩 이미지를 불러올 때, 순서를 섞을지 있는 순서대로 불러 올지\n",
    "    - drop_last: batch_size만큼 잘라서 불러올 때 맨 뒤에 숫자가 맞지 않게 남는 데이터들을 어떻게 할 것이냐?\n",
    "    \n",
    "for 문을 통해서 데이터를 불러 온다. X에는 MNIST 이미지, Y는 label (0-9)\n",
    "\n",
    "view()를 이용해서 28*28을 784로 바꿔 준다.\n",
    "\n",
    "처음 X는 (배치 사이즈 크기 B, 1 채널, 28 높이, 28 너비)로 불러오는데, view를 이용해서, (배치 사이즈 크기 B, 784)로 바꿔주게 됨.\n",
    "이걸 다시 X에 저장해서 사용하게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e187a31d-91d7-40f5-9bbb-1b84fa382b13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5f/hjkz04pn6gj7gkf81d1lrfgh0000gq/T/ipykernel_18911/3067507320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmnist_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "...\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "data_loader = Dataset(DataLoader=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "...\n",
    "for epoch in range(training_epochs):\n",
    "    ...\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # Label is not one-hot encoded\n",
    "        X = X.view(-1, 28*28).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b48c1-1013-493e-a651-7eaff5ec4407",
   "metadata": {},
   "source": [
    "## Epoch / Batch size / Iteration\n",
    "\n",
    "In the neural network terminology:\n",
    "- one **epoch** = one forward pass and one backward pass of *all* the training examples\n",
    "- **batch size** = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "- number of **iterations** = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "    \n",
    "**Example**: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n",
    "\n",
    "\n",
    "- MNIST train set은 60,000장으로 구성되어 있는데, 이 이미지들 전체가 트레이닝에 사용되었으면 한 **epoch**가 돌았다고 표현한다.\n",
    "- 근데 한 번에 돌리기엔 메모리 많이 필요하고, 속도도 오래 걸림. 그래서 잘라서 사용. 이 자르는 크기를 몇으로 할 거냐가 **batch size**\n",
    "- **iteration**은 이 batch를 몇 번 학습에 사용을 했냐?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02739b09-c68b-4795-adfd-dfb8b093d502",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "classifier를 학습하는 방법을 좀 더 자세히 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b0d5a98-9ab5-4d7c-b29a-927c97aa4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "    \n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "995a1348-e3b5-48a3-9dba-4d1f8eda4457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 0.535150588\n",
      "Epoch:  0002 cost = 0.359577775\n",
      "Epoch:  0003 cost = 0.331264287\n",
      "Epoch:  0004 cost = 0.316404670\n",
      "Epoch:  0005 cost = 0.307107002\n",
      "Epoch:  0006 cost = 0.300456554\n",
      "Epoch:  0007 cost = 0.294933379\n",
      "Epoch:  0008 cost = 0.290956199\n",
      "Epoch:  0009 cost = 0.287074119\n",
      "Epoch:  0010 cost = 0.284515619\n",
      "Epoch:  0011 cost = 0.281914055\n",
      "Epoch:  0012 cost = 0.279526919\n",
      "Epoch:  0013 cost = 0.277636588\n",
      "Epoch:  0014 cost = 0.275874794\n",
      "Epoch:  0015 cost = 0.274422765\n"
     ]
    }
   ],
   "source": [
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)\n",
    "    # linear layer 사용. 0-9니까 10. bias(파라미터 b)는 true\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "# define cost/lost & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "    # PyTorch에서는 CrossEntropyLoss가 Softmax를 자동으로 계산해 주기 때문에 Softmax를 별도로 선언하지 않겠다.\n",
    "\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "    # Stochastic gradient descent. linear의 파라미터(w, b), learning rate는 0.1\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    # 15번 반복 학습\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "            # view를 이용해서 (배치 사이즈, 28 * 28)로 변경해서 사용.\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "            # linear 레이어에 입력으로 집어 넣으면, classifier가 분류한 분류 결과를 얻을 수 있을 것.\n",
    "        cost = criterion(hypothesis, Y)\n",
    "            # 분류 결과와 실제 정답 레이블 Y를 갖고 cross entropy를 이용해서 cost를 구하기.\n",
    "        cost.backward()\n",
    "            # 구해진 cost로, backward를 이용해서 gradient를 계산하게 되고,\n",
    "        optimizer.step()\n",
    "            # 그 gradient를 가지고 업데이트를 진행하게 된다.\n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a0d1f-77f6-4901-8372-bdf0429ff139",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9e8d1a8-209f-4a21-8ca1-62107678e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8883000016212463\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    # 이 범위 안에서는 grad 계산을 안 하겠다는 의미\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        # argmax: 최댓값이 있는 인덱스를 찾을 때 사용. (특히 Softmax와 같은 상황에서)\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b74a02-af5a-4127-85ed-aa8da9a34c6a",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e115b07-25a0-47b4-8889-cf342af4aa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  3\n",
      "Prediction:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "print('Label: ', Y_single_data.item())\n",
    "single_prediction = linear(X_single_data)\n",
    "print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
