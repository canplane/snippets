{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f0c31e-fe7e-4cdc-94fd-1be460113682",
   "metadata": {},
   "source": [
    "# Lab-10-4 ImageFolder\n",
    "\n",
    "[1]\n",
    "\n",
    "## 지난시간에는\n",
    "- Visdom 사용법 및 적용\n",
    "\n",
    "## 오늘은!\n",
    "- 내가 가진 사진을 이용해서 딥러닝 모델 학습해보기\n",
    "\n",
    "## 진행순서\n",
    "1. 나만의 데이터 셋 준비하기\n",
    "2. torchvision.datasets.ImageFolder으로 불러오기\n",
    "3. transforms 적용하여 저장 하기 origin_data -> train_data\n",
    "\n",
    "## 나만의 데이터 셋 준비하기\n",
    "- 저는 connect 재단에 있던 의자를 딥러닝 모델로 구분해보려고 해요\n",
    "- 여러분도 여러분의 사진을 준비해보세요!\n",
    "- 연습이니까 **시각적으로 명확하게 구분되는 사진**을 사용하시는 걸 **추천** 드립니다.\n",
    "\n",
    "![snapshot](./10-4-1-0.png)\n",
    "\n",
    "![snapshot](./10-4-1-1.png)\n",
    "\n",
    "![snapshot](./10-4-1-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de1cc5-11c5-45e1-a498-c806415ddb36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 직접해보기\n",
    "- jupyter notebook으로~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d11c575-13c9-4f3c-a74e-90ab32092b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d477fa1-d827-4952-abd7-d20fbdb830c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# matplotlib의 출력값이 주피터 노트북 상에 표현되게 해주세요. 인라인으로 해달라. 이런 말.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038cf97d-cdf5-4b01-b1bd-f998f8c69403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-1-1_numpy.ipynb    07-2.ipynb            10-2-0.png\n",
      "01-1-2_torch.ipynb    08-1-perceptron.ipynb 10-2-1.png\n",
      "01-2.ipynb            08-2-mlp.ipynb        10-2.ipynb\n",
      "02.ipynb              09-1.ipynb            10-3.ipynb\n",
      "03.ipynb              09-2.ipynb            10-4-1-0.png\n",
      "04-1.ipynb            09-3.ipynb            10-4-1-1.png\n",
      "04-2.ipynb            09-4.ipynb            10-4-1-2.png\n",
      "05.ipynb              10-1-0.png            10-4-1.ipynb\n",
      "06.ipynb              10-1-1.png            \u001b[34mMNIST_data\u001b[m\u001b[m\n",
      "07-1.ipynb            10-1.ipynb            \u001b[34mcifar10\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# 먼저 수행하기 전에, 내가 있는 위치의 어디에 데이터셋이 있는지를 확인해봐야 함.\n",
    "# ![COMMAND] : e.g., !ls custom_data/origin_data -> \"gray red\" (folders)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd78c83-05f2-481f-aa67-af4c24ab1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = torchvision.datasets.ImageFolder(root=\"custom_data/origin_data\", transform=None)\n",
    "\n",
    "# transform을 많이 수행해야 되면 그걸 일일이 여기에다 넣어주는 게 귀찮다. 그래서 그걸 하나로 묶어주는 역할.\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((64, 128))\n",
    "])\n",
    "train_data = torchvision.datasets.ImageFolder(root=\"custom_data/origin_data\", transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e349f8-92be-4d69-a8e5-d81cf784a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, value in enumerate(train_data):\n",
    "    data, label = value\n",
    "    print(num, data, label)\n",
    "    \n",
    "    # trans한 data를 다시 저장\n",
    "    if label == 0:  # 0(gray) 아니면 1(red) 밖에 라벨이 없기 때문에 이렇게 진행하겠다.\n",
    "        data.save('custom_data/train_data/gray/%d_%d.jpeg' % (num, label))\n",
    "    else:\n",
    "        data.save('custom_data/train_data/red/%d_%d.jpeg' % (num, label))\n",
    "    \n",
    "    '''\n",
    "    imshow(data)   # image 출력\n",
    "    break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa727bb9-c4a3-4195-8adc-c1f57036dbb5",
   "metadata": {},
   "source": [
    "![snapshot](./10-4-1-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5bace4-5443-44ff-a1c8-e93d4bf0c872",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 오늘 같이 해본 것들은?\n",
    "- 내가 가진 사진을 이용해서 dataset을 만드는 법\n",
    "\n",
    "## What's Next?\n",
    "- CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d155e34-e905-4e54-8fc3-464ce7b97f50",
   "metadata": {},
   "source": [
    "[2]\n",
    "\n",
    "## 지난시간에는\n",
    "- ImageFolder 사용법\n",
    "\n",
    "## 오늘은\n",
    "- 내 사진을 분류하는 딥러닝 모델 학습\n",
    "\n",
    "![snapshot](./10-4-2-0.png)\n",
    "\n",
    "*[16, 13, 29] : (C, H, W)*\n",
    "\n",
    "## 오늘 같이 해볼 것들은?\n",
    "- 내가 가진 사진을 이용해서 dataset을 만드는 법\n",
    "- 배운 내용을 이용해서 학습하기\n",
    "- 모델을 저장하고 다시 불러오는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892ee71-7d58-4559-964f-31c8f35fa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root='./custom_data/train_data', transform=trans)\n",
    "\n",
    "\n",
    "\n",
    "data_loader = DataLoader(dataset=train_data, batch_size=8, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16*13*29, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 일반적으로 레이어 사이즈 바꾸게 되면 이 부분에서 귀찮게 되는 경우가 있음.\n",
    "        # 편하게 할 수 있는 방법은 아래처럼 shape 넣어서 테스트하는 게 가장 좋은 방법이다.\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        #print(out.shape) # 이렇게 하면 각 레이어 통과해서 크기 어떻게 되는지 바로바로 잡혀서 좋다.\n",
    "        out = self.layer2(out)\n",
    "        #print(out.shape)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        #print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "#testing\n",
    "net = CNN().to(device)\n",
    "test_input = (torch.Tensor(3,3,64,128)).to(device)\n",
    "test_out = net(test_input)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "\n",
    "epochs = 5   # 데이터셋이 워낙 쉽기 때문에 잘 학습이 되니까 3번만 돌리자.\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0.0\n",
    "    \n",
    "    for num, data in enumerate(data_loader):\n",
    "        imgs, labels = data\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = net(imgs)\n",
    "        loss = loss_func(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += loss / total_batch\n",
    "    \n",
    "    print('[Epoth:{}] cost = {}'.format(epoch+1, avg_cost))\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ebebf-2801-4cae-af72-a246dbe80234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 save해서 다시 불러오는 방법\n",
    "torch.save(net.state_dict(), \"./model/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32651d2-a8c4-4a45-a4c5-4a33d22a3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net = CNN().to(device)\n",
    "new_net.load_state_dict(torch.load('./model/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe03289-9e21-4b3f-998c-6ed1ef0bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 게 같은지 확인 한번 해본 것\n",
    "\n",
    "print(net.layer1[0])\n",
    "print(new_net.layer1[0])\n",
    "\n",
    "print(net.layer1[0].weight[0][0][0])\n",
    "print(new_net.layer1[0].weight[0][0][0])\n",
    "\n",
    "net.layer1[0].weight[0] == new_net.layer1[0].weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae788f2a-bab8-4486-a3db-f3d4d8eb88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torchvision.transforms.Compose([\n",
    "    transforms.Resize((64, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_data = torchvision.datasets.ImageFolder(root='./custom_data/test_data', transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673286d-a454-4c3e-8070-4202b1bffe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = DataLoader(dataset=test_data, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16113b-7b1c-48eb-b997-cd834b835ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for num, data in enumerate(test_set):\n",
    "        imgs, label = data\n",
    "        imgs = imgs.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        prediction = net(imgs)\n",
    "        \n",
    "        correct_prediction = torch.argmax(prediction, 1) == label\n",
    "        \n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db854d-588a-4a9c-9b25-60f65756978c",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "- Advanced CNN (AlexNet, VGGNet, ResNet, DenseNet, SENet, AutoML, ...)\n",
    "    - 하나하나 다 설명하긴 힘들고, 이전에 올린 강의에 이론 있고, 여기서는 실습 위주로 많이 다뤄보려고 함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
