{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc52d2f-f789-4211-806a-a6bd2cbda104",
   "metadata": {},
   "source": [
    "# Lab-08-2 Multi Layer Perceptron\n",
    "\n",
    "- Review: XOR\n",
    "- Multi Layer Perceptron\n",
    "- Backpropagation\n",
    "- Code: xor-nn\n",
    "- Code: xor-nn-wide-deep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f2c7f-df66-419f-a70c-7adacec2adff",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "Input/Hidden/Output layer\n",
    "\n",
    "\n",
    "- We need to use MLP, multilayer perceptrons (multilayer neural nets)\n",
    "\n",
    "- No one on earth had found a viable way to train MLPs good enough to learn such simple functions (그래서 뉴럴 네트워크로는 xor 문제를 푸는 게 불가능하다고 생각하게 된 것.)\n",
    "\n",
    "(By Marvin Minsky, founder of the MIT AI Lab, 1969)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4bd464-5105-40b5-96bd-b419ce843756",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "(X1, X2, ...) => ... => Y1\n",
    "\n",
    "output $O$, 원래 정답 $G(t)$에 대해 $Loss = O - G(t)$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial W}$\n",
    "\n",
    "**뉴럴 네트워크에 있는 weight들에 대한 미분 값을 계산하게 되고, 이 gradient를 가지고 뒷단에 있는 weight부터 loss 값을 최소화시킬 수 있도록 weight를 업데이트하는 방식을 backpropagation 알고리즘이라 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84a926-01f1-4003-be62-40c0fa29149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "# NN 학습 위해 레이어 선언해야 하는데, 원래는 nn.Linear 레이어를 주로 사용했지만 이번엔 weight, bias 직접 선언해보겠다.\n",
    "# nn layers\n",
    "w1 = torch.Tensor(2, 2)\n",
    "b1 = torch.Tensor(2)\n",
    "w2 = torch.Tensor(2, 1)\n",
    "b2 = torch.Tensor(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # sigmoid function\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "    # return torch.div(torch.tensor(1), torch.add(torch.tensor(1.0), torch.exp(-x)))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    # derivative of the sigmoid function\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccbb47-a654-4ae4-b99e-6a62fdb4bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(10001):\n",
    "    \n",
    "    # forward\n",
    "    \n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)    # l1 = X x w1 + b1\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)   # l2 = a1 x w2 + b2\n",
    "    Y_pred = sigmoid(l2)                       # Y prediction\n",
    "    \n",
    "    cost = -torch.mean(Y * torch.log(Y_pred) + (1 - Y) * torch.log(1 - Y_pred))\n",
    "        # binary cross entropy\n",
    "    \n",
    "    \n",
    "    # Back prop (chain rule)\n",
    "    \n",
    "    # Loss derivative\n",
    "    d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)    # dLoss/dy (y는 예측 값)\n",
    "        # binary cross entropy를 미분한 식. 1e-7은 0으로 나누어지는 경우를 막아주기 위한 term\n",
    "    \n",
    "    # Layer 2\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)                          # dLoss/dl2 = (dLoss/dy)(dy/dl2)\n",
    "    d_b2 = d_l2                                                  # dLoss/db2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)         # dLoss/dw2, (2, 4) x (4, 1) = (2, 1)\n",
    "    \n",
    "    # Layer 1\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
    "    \n",
    "    \n",
    "    # Weight update\n",
    "    \n",
    "    w1 = w1 - learning_rate * d_w1\n",
    "    b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
    "    w2 = w2 - learning_rate * d_w2\n",
    "    b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd83c3-19bc-4b54-ae40-269064e7905b",
   "metadata": {},
   "source": [
    "## Code: xor-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1b83c3-cc24-4be1-b544-e67e1610a76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7624428272247314\n",
      "100 0.6916636824607849\n",
      "200 0.6850571632385254\n",
      "300 0.6277700662612915\n",
      "400 0.5413362979888916\n",
      "500 0.39542973041534424\n",
      "600 0.12126626074314117\n",
      "700 0.05854424089193344\n",
      "800 0.03754853457212448\n",
      "900 0.027400746941566467\n",
      "1000 0.021485967561602592\n",
      "1100 0.017632946372032166\n",
      "1200 0.01493168156594038\n",
      "1300 0.012936640530824661\n",
      "1400 0.011404736898839474\n",
      "1500 0.010192646645009518\n",
      "1600 0.009210258722305298\n",
      "1700 0.008398361504077911\n",
      "1800 0.007716350723057985\n",
      "1900 0.007135570980608463\n",
      "2000 0.006635206285864115\n",
      "2100 0.006199636030942202\n",
      "2200 0.005817198660224676\n",
      "2300 0.005478763021528721\n",
      "2400 0.005177151411771774\n",
      "2500 0.0049066971987485886\n",
      "2600 0.004662837367504835\n",
      "2700 0.004441898316144943\n",
      "2800 0.0042407214641571045\n",
      "2900 0.004056855104863644\n",
      "3000 0.003888150444254279\n",
      "3100 0.0037327734753489494\n",
      "3200 0.003589252708479762\n",
      "3300 0.0034562810324132442\n",
      "3400 0.0033327334094792604\n",
      "3500 0.0032176636159420013\n",
      "3600 0.0031101868953555822\n",
      "3700 0.003009598469361663\n",
      "3800 0.0029152974020689726\n",
      "3900 0.0028266701847314835\n",
      "4000 0.0027432364877313375\n",
      "4100 0.0026645762845873833\n",
      "4200 0.0025902108754962683\n",
      "4300 0.002519959583878517\n",
      "4400 0.002453299006447196\n",
      "4500 0.002390108536928892\n",
      "4600 0.002330014482140541\n",
      "4700 0.0022729113698005676\n",
      "4800 0.002218469977378845\n",
      "4900 0.0021665706299245358\n",
      "5000 0.002117063384503126\n",
      "5100 0.0020697240252047777\n",
      "5200 0.0020244328770786524\n",
      "5300 0.001981099834665656\n",
      "5400 0.001939575420692563\n",
      "5500 0.0018997399602085352\n",
      "5600 0.0018614588771015406\n",
      "5700 0.0018247468397021294\n",
      "5800 0.0017893947660923004\n",
      "5900 0.00175540242344141\n",
      "6000 0.0017226503696292639\n",
      "6100 0.0016911084530875087\n",
      "6200 0.0016607168363407254\n",
      "6300 0.0016313563100993633\n",
      "6400 0.001603026408702135\n",
      "6500 0.0015756527427583933\n",
      "6600 0.001549205044284463\n",
      "6700 0.001523608691059053\n",
      "6800 0.0014988341135904193\n",
      "6900 0.0014748659450560808\n",
      "7000 0.0014516296796500683\n",
      "7100 0.0014291550032794476\n",
      "7200 0.0014073224738240242\n",
      "7300 0.0013861621264368296\n",
      "7400 0.0013655840884894133\n",
      "7500 0.0013456629822030663\n",
      "7600 0.0013263095170259476\n",
      "7700 0.0013074486050754786\n",
      "7800 0.0012891553342342377\n",
      "7900 0.001271384535357356\n",
      "8000 0.0012540466850623488\n",
      "8100 0.001237216405570507\n",
      "8200 0.001220789272338152\n",
      "8300 0.001204795204102993\n",
      "8400 0.0011892340844497085\n",
      "8500 0.001174060977064073\n",
      "8600 0.0011592312948778272\n",
      "8700 0.0011448344448581338\n",
      "8800 0.0011307511013001204\n",
      "8900 0.0011170260841026902\n",
      "9000 0.00110361457336694\n",
      "9100 0.001090501551516354\n",
      "9200 0.0010777616407722235\n",
      "9300 0.0010652607306838036\n",
      "9400 0.0010530431754887104\n",
      "9500 0.0010411391267552972\n",
      "9600 0.001029459061101079\n",
      "9700 0.0010180623503401875\n",
      "9800 0.0010069046402350068\n",
      "9900 0.0009959856979548931\n",
      "10000 0.0009853055234998465\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid)\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675fe5ee-77d6-49f7-92ab-647635acfb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[9.4024529e-04]\n",
      " [9.9870014e-01]\n",
      " [9.9912483e-01]\n",
      " [8.2351483e-04]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c2629-bd9e-451b-87c5-f430d5d7c2f8",
   "metadata": {},
   "source": [
    "## Code: xx-nn-wide-deep\n",
    "\n",
    "조금 더 레이어를 깊게 쌓아보자. 2개 더 쌓아서 4개."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fcee3f-01a5-4f17-8fec-600d00c285bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
    "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8dd01e-f078-47d9-a607-ec6adedbfb00",
   "metadata": {
    "tags": []
   },
   "source": [
    "이러면 훨씬 더 낮은 loss 값 보임!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3cafa-fc5e-4c65-ad3b-21d49fb6bcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
