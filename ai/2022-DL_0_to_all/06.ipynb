{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06ab7dc-f478-4e4e-b33e-65e4c526e20f",
   "metadata": {},
   "source": [
    "# Lab-06 Softmax Classification\n",
    "- Softmax\n",
    "- Cross Entropy\n",
    "- Low-level Implementation\n",
    "- High-level Implementation\n",
    "- Training Example\n",
    "\n",
    "Logistic Regression의 연장선상이라고 이해해도 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47c6abe-9d59-422c-b43c-49ec78a15432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50daa419-0fa6-4a00-981f-32e33800599e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcdf87a1f30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For repreducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea7c2f-6280-4137-950d-08c5389fcf98",
   "metadata": {},
   "source": [
    "## Discrete Probability Distribution\n",
    "\n",
    "이산 확률 분포\n",
    "\n",
    "\n",
    "$P(X=가위)=0.2$, $P(X=6)=\\frac{1}{6}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee144fa9-8545-4b49-a7ca-7e0e9e5a9dc6",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Convert numbers to probabilities with softmax.\n",
    "\n",
    "max 값을 뽑아 주는데, soft하게 뽑아준다는 의미\n",
    "\n",
    "$P(class=i)=\\frac{e^i}{\\sum{e^i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90984337-547e-4d86-a000-fdc4b2e7e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b4a13-d1f1-4baf-9cdc-d8a439804f30",
   "metadata": {},
   "source": [
    "PyTorch has a `softmax` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b3b69f-76eb-4618-adb2-2738d699f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dadee6-b4da-4f58-9401-3f31ea9cb1f5",
   "metadata": {},
   "source": [
    "원래는 $max=(0, 0, 1)$이어야 하겠지만 soft하게 뽑아주므로 $softmax=(0.0900, 0.2447, 0.6652)$\n",
    "\n",
    "$(0.09 = \\frac{e^1}{e^1 + e^2 + e^3})$\n",
    "\n",
    "\n",
    "ex) 예전에 가위를 냈을 때 주먹을 낼 확률 -> $P(주먹 | 가위)$, $P(가위 | 가위)$, $P(보 | 가위)$\n",
    "\n",
    "Since they are probabilities, they should add up to 1. Let's do a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d545c960-e8cd-42ba-a6e7-ff881d1b638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()   # 합은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb674c8-e917-4cf0-a84b-3a194fd2e9c4",
   "metadata": {},
   "source": [
    "For multi-class classification, we use the cross entropy loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a4835-69db-4d0f-814c-c2575177584e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Entropy\n",
    "\n",
    "$H(P, Q) = -\\mathbb{E}_{x \\sim P(x)}[\\log Q(x)] = - \\sum_{x \\in X}P(x) \\log Q(x)$\n",
    "\n",
    "Cross Entropy를 최소화하는 것(-> 0)이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9896f5-6eee-4f83-a603-66d9daed8d17",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss (Low-level)\n",
    "\n",
    "For multi-class classification, we use the cross entropy loss.\n",
    "\n",
    "$L = \\frac{1}{N} \\sum -y \\log (\\hat{y})$\n",
    "\n",
    "y -> P(x) : 실제 y, y hat -> Q(x) : P_theta(x)\n",
    "\n",
    "where $\\hat{y}$ is the predicted probability and $y$ is the correct probability (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c2efb6-a151-4194-b783-b19bd9b795f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1346, 0.2520, 0.1707, 0.2451, 0.1975],\n",
      "        [0.2180, 0.2065, 0.2219, 0.2371, 0.1164],\n",
      "        [0.2781, 0.1521, 0.1483, 0.2229, 0.1986]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# #classes = 5, #samples = 3\n",
    "z = torch.rand(3, 5, requires_grad=True)    # |z| = (3, 5), 그래디언트 학습하기\n",
    "hypothesis = F.softmax(z, dim=1)            # prediction = y hat\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3c8ded7-23ca-4739-8363-4e43f64b0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()  # 따로 주어진 정답이 없으니, 정답을 랜덤하게 생성해 보자.\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2320e512-a4fa-4881-b72e-59d4c6764738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)   # |y_one_hot| = (3, 5)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)   # y.unsqueeze(1)을 dim=1로 1을 뿌려라! _는 in-place\n",
    "# |y| = (3,) => |y.unsqueeze(1)| = (3, 1)\n",
    "# y = [0, 2, 1] => y.unsqueeze(1) = [0, 2, 1]^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98b2738b-48e6-4d3a-bc02-495ffa8abd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5975, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "# y_one_hot * -torch.log(hypothesis) => (3, 5)\n",
    "# .sum(dim=1) => (3,)\n",
    "# .mean() => scalar\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a86e7-45ee-4080-a62f-30ed380b8e7f",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss with torch.nn.functional\n",
    "\n",
    "위의 것은 low-level이니 좀 더 편리한 모듈 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1974bbf-3800-4cdf-aafd-6e9612ea119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0052, -1.3784, -1.7676, -1.4061, -1.6218],\n",
       "        [-1.5231, -1.5773, -1.5055, -1.4394, -2.1503],\n",
       "        [-1.2797, -1.8830, -1.9087, -1.5010, -1.6166]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.log_softmax(z, dim=1)    # Low level : torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88d030ab-df78-4c05-956b-4625d362840c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5975, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low level\n",
    "#(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "# High level\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)   # NLL = Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734f9ac-116f-4ddb-9d28-49b3e0732dc7",
   "metadata": {},
   "source": [
    "**PyTorch also has `F.cross_entropy` that combines `F.log_softmax()` and `F.nll_loss()`!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "947a10b9-d520-4dee-9317-b952c5418a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5975, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe723294-3aec-483d-89cb-788113983062",
   "metadata": {},
   "source": [
    "## Training with Low-level Cross Entropy Loss\n",
    "\n",
    "#samples = m, #classes = 3, dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9c2bc64-6889-438c-9f8e-e518334dd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 5],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "\n",
    "# |x_train| = (m, 4)\n",
    "# |y_train| = (m,)\n",
    "\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50314476-f349-47fb-93ba-2c11b7edb5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.775065\n",
      "Epoch  200/1000 Cost: 0.712725\n",
      "Epoch  300/1000 Cost: 0.677388\n",
      "Epoch  400/1000 Cost: 0.650680\n",
      "Epoch  500/1000 Cost: 0.627860\n",
      "Epoch  600/1000 Cost: 0.607082\n",
      "Epoch  700/1000 Cost: 0.587473\n",
      "Epoch  800/1000 Cost: 0.568567\n",
      "Epoch  900/1000 Cost: 0.550100\n",
      "Epoch 1000/1000 Cost: 0.531914\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)   # (4, 3)이므로 4 samples -> 3 classes\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # Cost 계산\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)   # or .mm or @\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} Cost: {:6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7b835-6fd0-4ab9-a876-b39229eca315",
   "metadata": {},
   "source": [
    "## Training with F.cross_entropy\n",
    "\n",
    "cross_entropy()를 쓰면 좀 더 간단해짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "616e2ed5-6b98-4709-8bb2-752d885fe7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.775066\n",
      "Epoch  200/1000 Cost: 0.712725\n",
      "Epoch  300/1000 Cost: 0.677388\n",
      "Epoch  400/1000 Cost: 0.650680\n",
      "Epoch  500/1000 Cost: 0.627860\n",
      "Epoch  600/1000 Cost: 0.607082\n",
      "Epoch  700/1000 Cost: 0.587473\n",
      "Epoch  800/1000 Cost: 0.568567\n",
      "Epoch  900/1000 Cost: 0.550100\n",
      "Epoch 1000/1000 Cost: 0.531914\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000                               # 1000번 epoch 돌리고\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # Cost 계산 (2)\n",
    "    z = x_train.matmul(W) + b  # or .mm or @   # 여기까진 똑같다.\n",
    "    cost = F.cross_entropy(z, y_train)         # 다만 cost 구할 때, z, y 통해 실제 정답 바로 비교. (scatter 필요 없음.)\n",
    "                                               # 즉, one hot vector 필요 없다.\n",
    "    \n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()                      # optimizer에 zero grad를 하고,\n",
    "    cost.backward()                            # back propagation하고,\n",
    "    optimizer.step()                           # back prop.해서 얻은 gradient대로 다시 한번 grad. descent 통해서\n",
    "                                               # cross entropy 함수를 minimize하돌고 한다.\n",
    "                                               # minimize해서 어떤 grad truth 확률 분포에 좀 더 근사!\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} Cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f7f0e-bb71-42b1-a043-6bb311ca151e",
   "metadata": {},
   "source": [
    "## High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e000fd99-c2de-4d21-942e-f5e6518ab0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3)   # Output이 3! (4 => 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)           # |x| = (m, 4) => output (m, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1af550af-e210-48bc-b7fd-43eab0228f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c81c0211-665e-439f-9f8d-55365a27ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.626739\n",
      "Epoch  100/1000 Cost: 0.664001\n",
      "Epoch  200/1000 Cost: 0.582189\n",
      "Epoch  300/1000 Cost: 0.533072\n",
      "Epoch  400/1000 Cost: 0.494812\n",
      "Epoch  500/1000 Cost: 0.461731\n",
      "Epoch  600/1000 Cost: 0.431563\n",
      "Epoch  700/1000 Cost: 0.403123\n",
      "Epoch  800/1000 Cost: 0.375636\n",
      "Epoch  900/1000 Cost: 0.348516\n",
      "Epoch 1000/1000 Cost: 0.321313\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # 모델의 파라미터 즉 linear 레이어 하나 들어있는 건데 그걸 SGD\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)     # model에 통과. in: |x_train| = (m, 4) => out: |prediction| = (m, 3)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)  # |y_train| = (m,) (인덱스 값, 즉 아직 원핫벡터로 바꾸지 않은 상태)\n",
    "                                                 # cost 계산하면 어떤 스칼라 값 나올 것.\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()         # back propagation 수행하면 gradient descent가 model.parameters()에 채워졌겠지\n",
    "    optimizer.step()        # 그러니 우리는 거기에 대해 gradient descent 1 step을 수행. 이걸 1000번 수행\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} Cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a8dee-96f2-4df0-886e-a6d296faa237",
   "metadata": {},
   "source": [
    "**보면 알겠지만, softmax classification은 logistic regression과 굉장히 유사.**\n",
    "\n",
    "logi. regr.는 0과 1의 두 가지 class만 있는 discrete한 class가 있는 classification (binary classification),\n",
    "softmax classification의 경우는, multinary, multinomial한 확률 분포가 되겠다.\n",
    "\n",
    "그래서 추후 딥러닝할 때 보통 습관적으로 어떤 classification을 하기 위해서는, cross entropy를 써야 하고,\n",
    "softmax layer를 쓰게 되기 마련인데, 사실, binary classification인 경우의 문제도 많다.\n",
    "\n",
    "그럴 때는 logistic regression에서 사용한 loss 함수를 사용하는 것이 맞다.\n",
    "즉, binary cross entropy (BCE)를 사용하는 것이 맞다. 그리고 sigmoid를 사용해야겠지.\n",
    "\n",
    "즉,\n",
    "\n",
    "- binary classification\n",
    "    - BCE\n",
    "    - sigmoid\n",
    "    \n",
    "- multinary classification (지금처럼 여러 개의 클래스가 있을 때)\n",
    "    - CE\n",
    "    - softmax layer\n",
    "    \n",
    "이러한 부분에 있어 추후 딥러닝을 직접 구현할 때 있어 주의하기 바람."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
